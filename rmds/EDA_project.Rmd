---
title: "EDA_project"
author: "Alexis Rangel"
date: "2023-04-16"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, cargando Rdata, include=FALSE}
load("~/Maestria/1er semestre/Modelos lineales/Proyecto final de curso Modelos Lineales MAEA 1er semestre/brief/DatosParaProyecto.RData")
Train_copy = Train
Test_copy = Test
```

```{r, include=FALSE}
library(tidyverse)
library(explore)
library(tidyr)
library(gridExtra)
library(glue)
library(PerformanceAnalytics)
library(dataxray)
library(correlationfunnel)
library(Amelia)
library(RColorBrewer)
library(paletteer)
```


## Análisis exploratorio

Sobre la base:

La base de datos consta de 18 variable, una variable a predecir y 17 candidatas a predictoras.
34209 registros de cada variable.


1. Loan.Status (VARIABLE DICOTOMA A PREDECIR, VALORES POSIBLES -> [Fully Paid, Charged Off])

Nombre variable | tipo de variable | num valores posibles | valores posibles

2. Term | nominal | 2 | ["Long Term",  "Short Term"]
3. Years.in.current.job | ordinal | 12 | ["n/a", "< 1 year", "1 year":"9 years", "10+ years"]
4. Home.Ownership | nominal | 4 | ["Rent", "Home Mortgage", "Own Home", "HaveMortgage" ]
5. Purpose | nominal | 16 | [...>15 valores posibles...] | << Requiere homologar como factor "other" and "Other" >>
6. Current.Loan.Amount | continua | [min -> 21449.74	, mean -> 14044734.38, max -> 100000002.5]
7. Credit.Score	 | continua | [min -> 584.00, mean -> 1048.48, max -> 7509.0]
8. Annual.Income	| continua | [min -> 164596.55, mean -> 1441110.95, max -> 30838993.9]
9. Monthly.Debt	 | continua | [min -> 0.00, mean -> 19025.56, max -> 229056.4]
10. Years.of.Credit.History	 | continua | [min -> 2.30, mean -> 19.21, max -> 59.6]
11. Months.since.last.delinquent | continua | [min-> 0.00, mean -> 34.96, max -> 178.1]
(Meses desde el último moroso)
12. Number.of.Open.Accounts	 | continua | [min ->  0.00, mean -> 11.43, max -> 49.0]
13. Number.of.Credit.Problems	 | continua | [min -> 0.00, mean -> 0.53, max -> 15.0
]
14. Current.Credit.Balance	| continua | [min ->  0.00, mean -> 259532.60, max -> 7140732.6]
15. Maximum.Open.Credit	| continua | [min ->  0.00, mean -> 666270.83, max -> 798255369.7]
16. Bankruptcies | continua | [min ->  0.00, mean -> 0.46, max -> 7.0]
17. Tax.Liens	 | continua | [min ->  0.00, mean -> 0.42, max -> 14.0]
(gravámenes fiscales)
18. ID ---- excluir variable de cualquier análisis ----

Conteos:
nominal - 3
ordinal - 1
continua - 8

Sobre la variable dependiente y de acuerdo con la siguiente pagina web:

**A charge-off is the opposite of paid in full. It means the lender hasn't received payment for at least 180 days, and the account is in default. The lender, or a third-party collection agency, can still come after this kind of debt. Charge-offs have an extremely negative effect on your credit score.**

Fuente: https://budgeting.thenest.com/account-paid-full-vs-chargeoff-23884.html

#### Valores faltantes

```{r, echo=FALSE}
missmap(Train_copy, main = "Missing values vs observed")
```

#### Análisis univariado



- Observamos la distribución desbalanceada de los registros a clásificar, en donde `Fully Paid` representa el 79.23% de la respuesta dicotómica.
```{r, echo=FALSE}
table(Train_copy$Loan.Status)
```


Inicialmente pasamos a binario el pago (`Fully Paid`) o el default (`Charged Off`) con un 1 y un 0 respectivamente.

```{r, echo=FALSE}
Train_copy$Loan.Status = ifelse(Train_copy$Loan.Status  == "Fully Paid", 1, 0)
Train_copy$Loan.Status = as.factor(Train_copy$Loan.Status)
```

- Observamos la variable respuesta `Loan.Status`:
```{r, echo=FALSE}
ggplot(Train_copy, aes(fct_infreq(Loan.Status)))+
  geom_bar(stat="count", fill=c("steelblue", "firebrick"))+
  labs(title = "Variable 'Loan.Status'",
       x = "Plazo",
       y = "Frecuencia") +
  theme_bw() +
  # Agregar el número de observaciones al filo de cada barra
  geom_text(stat = "count", aes(label = after_stat(format(count, big.mark = ","))), vjust = -0.5)
```

######  Análisis univariado variables nominales y ordinales

Paso numero 1, pasar a factores todas estas variables nominales y ordinales

a. Term
```{r, echo=FALSE}
#class(Train_copy$Term)
Train_copy$Term = as.factor(Train_copy$Term)

ggplot(Train_copy, aes(fct_infreq(Term)))+
  geom_bar(stat="count", fill=c("darkgreen", "yellowgreen"))+
  labs(title = "Variable 'Term'",
       x = "Plazo",
       y = "Frecuencia") +
  theme_bw() +
  # Agregar el número de observaciones al filo de cada barra
  geom_text(stat = "count", aes(label = after_stat(format(count, big.mark = ","))), vjust = -0.5)
```

b. Years.in.current.job
```{r, echo=FALSE}
Train_copy$Years.in.current.job = as.factor(Train_copy$Years.in.current.job)
my_palette = paletteer_c("ggthemes::Blue", length(unique(Train_copy$Years.in.current.job)))
my_palette_reverse = rev(my_palette)

ggplot(Train_copy, aes(fct_infreq(Years.in.current.job)))+
  geom_bar(stat="count", fill=my_palette_reverse)+
  labs(title = "Variable 'Years.in.current.job'",
       x = "Años",
       y = "Frecuencia") +
  theme_bw() +
  # Agregar el número de observaciones al filo de cada barra
  geom_text(stat = "count", aes(label = after_stat(format(count, big.mark = ","))), vjust = -0.5)

```

c. Home.Ownership
```{r, echo=FALSE}
Train_copy$Home.Ownership = as.factor(Train_copy$Home.Ownership)

my_palette = paletteer_c("ggthemes::Blue", length(unique(Train_copy$Home.Ownership)))
my_palette_reverse = rev(my_palette)

ggplot(Train_copy, aes(fct_infreq(Home.Ownership)))+
  geom_bar(fill=my_palette_reverse)+
  labs(title = "Variable 'Home.Ownership'",
       x = "Tipo de dueño",
       y = "Frecuencia") +
  theme_bw() +
  # Agregar el número de observaciones al filo de cada barra
  geom_text(stat = "count", aes(label = after_stat(format(count, big.mark = ","))), vjust = -0.5)
```



d. Purpose

Por un lado, se unifica el valor de la variable "Other" y "other" en purpose con la funcion tolower(), y por otro, se valorará en el ajuste del modelo la reducción de dimensiones en Train_copy
```{r, echo=FALSE}
unique(Train_copy$Purpose)
Train_copy$Purpose = tolower(Train_copy$Purpose)

#unique(Train_copy$Purpose)
```

```{r, echo=FALSE}
Train_copy$Purpose = as.factor(Train_copy$Purpose)
my_palette = paletteer_c("ggthemes::Blue", length(unique(Train_copy$Purpose)))
my_palette_reverse = rev(my_palette)

ggplot(Train_copy, aes(fct_infreq(Purpose)))+
  geom_bar(stat="count", fill=my_palette_reverse)+
  labs(title = "Variable 'Purpose'",
       x = "Proposito",
       y = "Frecuencia") +
  theme_bw() +
  # Agregar el número de observaciones al filo de cada barra
  geom_text(stat = "count", aes(label = after_stat(format(count, big.mark = ","))), size=2)+
  coord_flip()

#unique(Train_copy$Purpose)
# # Cambiar "hombre" por 1 y "mujer" por 2
# nuevo_vector <- ifelse(observaciones == "hombre", 1, ifelse(observaciones == "mujer", 2, obse
```

######  Análisis univariado y agrupación grafica de variables continuas

Para el modelo de regresión logistica se plantean tres escenarios:

- Modelado con toda la muestra.
- Modelado con una muestra reducida (filtro en dos variables continuas)
- Modelado con una muestra aun más reducida (filtro en x variables continuas)

Todos, con su respectivo Train y Test, para la validación cruzada por medio del estadístico PRESS y el Cp de Mallow así como la validación cruzada por k-folds.

Para el modelado con muestras reducidas, se requiere un análisis estadístico univariado, bivariado y multivariado. Para esto y dado que son varias variables continuas, probablemente no tenga sentido meter todas en un solo gráfico de boxplot, por lo que buscamos rangos similares para segmentar en 2 o 3 grupos. Esto requiere inicialmente meter todas en un boxplot y manualmente separarlas


1er grupo de análisis:
`Years.of.Credit.History`, 
`Number.of.Open.Accounts` y
`Months.since.last.delinquent`

```{r, echo=FALSE}
Train_copy_continuas = Train_copy %>% select(Current.Loan.Amount, Credit.Score, Annual.Income, Monthly.Debt, Years.of.Credit.History, Months.since.last.delinquent, Number.of.Open.Accounts, Current.Credit.Balance, Maximum.Open.Credit, Bankruptcies, Tax.Liens)
```

```{r, echo=FALSE}
train_long_form = gather(select(Train_copy_continuas, -c(Maximum.Open.Credit, Current.Loan.Amount, Annual.Income, Current.Credit.Balance, Monthly.Debt, Credit.Score, Bankruptcies, Tax.Liens)), key = "variable", value = "valor")

ggplot(train_long_form, aes(y=variable, x=valor))+
  geom_boxplot()+
  # geom_jitter()+
  stat_summary(fun=mean, colour="darkred", geom="point",
               shape=18, size=3, show.legend=FALSE)+
  scale_x_continuous(NULL)+
  scale_y_discrete(NULL)+
  theme_bw()
```


De la anterior gráfica se puede apreciar la presencia de presuntos valores atípicos en las variables `Years.of.Credit.History`, `Number.of.Open.Accounts` y `Months.since.last.delinquent`. Un filtro IQRT podría ser la primera opción, pero se opta por hacer un análisis más univariante buscando adoptar un filtro que siga la lógica de negocio.

- `Years.of.Credit.History`

Visualizamos los cuartiles de la variable `Years.of.Credit.History`


```{r quantiles_, echo=FALSE}
quantiles_ <- function(var){
  vector <- Train_copy_continuas[[var]]
  print("Primer cuartil")
  print(quantile(vector, 0.25))
  #print("-----------")
  print("Segundo cuartil")
  print(quantile(vector, 0.5))
  #print("-----------")
  print("Tercer cuartil")
  print(quantile(vector, 0.75))
  #print("-----------")
  print("Cuarto cuartil")
  print(quantile(vector, 1))
}
```

```{r, echo=FALSE}
quantiles_("Years.of.Credit.History")
```
Destacar que el 75% de los valores del vector Current.Loan.Amount son menores o iguales a 22.7, mientras que el valor máximo es 59.6. El número de observaciones en el 4to cuartil es de 8429, por lo que un filtro eliminando toda esta submuestra nos haría perder una buena parte de la muestra. Por lo que decidimos hacer un análisis más a fondo para encontrar el punto en donde solo filtremos los valores que sean más extremos.

Con el apoyo visual de la siguiente gráfica pudimos decidir filtrar todas aquellas observaciones de la variable Years.of.Credit.History que sean mayores a 40. De esta forma, eliminariamos 401 variables de la muestra, representando un 1.17% de la misma.

```{r, echo=FALSE}
nrow(filter(Train_copy_continuas, Years.of.Credit.History > 40))
```

```{r, echo=FALSE}
plot_1 = ggplot(filter(Train_copy_continuas, Years.of.Credit.History <= 40), aes(y=1, x=Years.of.Credit.History))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  scale_y_continuous(NULL, labels = NULL)+
  ggtitle("Variable Years.of.Credit.History posterior al filtro (valores < 40)")+
  theme_bw()

plot_2 = ggplot(Train_copy_continuas, aes(y=1, x=Years.of.Credit.History))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  scale_y_continuous(NULL, labels = NULL)+
  ggtitle("Variable Years.of.Credit.History previo al filtro")+
  theme_bw()

grid.arrange(plot_2, plot_1, ncol = 1)
```

`Number.of.Open.Accounts`


Visualizamos los cuartiles para `Number.of.Open.Accounts`

```{r}
quantiles_("Number.of.Open.Accounts")
```
Destacar que el 75% de los valores del vector Number.of.Open.Accounts son menores o iguales a 14 cuentas abiertas. El número de observaciones en el 4to cuartil es de 7793, por lo que un filtro eliminando toda esta submuestra nos haría perder una buena parte de la muestra. Por lo que decidimos hacer un análisis más a fondo para encontrar el punto en donde solo filtremos los valores que sean más extremos.

Con el apoyo visual de la siguiente gráfica pudimos decidir filtrar todas aquellas observaciones de la variable Years.of.Credit.History que sean mayores a 30 De esta forma, eliminariamos 152 variables de la muestra, representando un 0.44% de la misma.
```{r, echo=FALSE, include=FALSE}
nrow(filter(Train_copy_continuas, Number.of.Open.Accounts > 30))
```

```{r, echo=FALSE}
plot_1 = ggplot(filter(Train_copy_continuas, Number.of.Open.Accounts <= 30), aes(y=1, x=Number.of.Open.Accounts))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  scale_y_continuous(NULL, labels = NULL)+
  # scale_x_continuous(breaks = c(0, 200000, 400000, 600000, 800000),
  #                    labels = c("0", "200k", "400k", "600k", "800k"))+
  ggtitle("Variable Number.of.Open.Accounts posterior al filtro (valores < 30)")+
  theme_bw()

plot_2 = ggplot(Train_copy_continuas, aes(y=1, x=Number.of.Open.Accounts))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  scale_y_continuous(NULL, labels = NULL)+
  # scale_x_continuous(breaks = c(0, 20000000, 40000000, 60000000, 80000000, 100000000),
  #                    labels = c("0", "20M", "40M", "60M", "80M", "100M"))+
  ggtitle("Variable Number.of.Open.Accounts previo al filtro")+
  theme_bw()

grid.arrange(plot_2, plot_1, ncol = 1)
```

- `Months.since.last.delinquent`

Visualizamos los cuartiles para `Months.since.last.delinquent`

```{r}
quantiles_("Months.since.last.delinquent")
```
Destacar que el 75% de los valores del vector Months.since.last.delinquent son menores o iguales a 51 meses, mientras que el valor máximo es 178. El número de observaciones en el 4to cuartil es de 8536, por lo que un filtro eliminando toda esta submuestra nos haría perder una buena parte de la muestra. Por lo que decidimos hacer un análisis más a fondo para encontrar el punto en donde solo filtremos los valores que sean más extremos.

Con el apoyo visual de la siguiente gráfica pudimos decidir filtrar todas aquellas observaciones de la variable Months.since.last.delinquent que sean mayores a 51 De esta forma, eliminariamos 31 variables de la muestra, representando un 0.09% de la misma.

```{r, echo=FALSE, include=FALSE}
nrow(filter(Train_copy_continuas, Months.since.last.delinquent > 85))
```

```{r, echo=FALSE}
plot_1 = ggplot(filter(Train_copy_continuas, Months.since.last.delinquent <= 85), aes(y=1, x=Months.since.last.delinquent))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  scale_y_continuous(NULL, labels = NULL)+
  # scale_x_continuous(breaks = c(0, 200000, 400000, 600000, 800000),
  #                    labels = c("0", "200k", "400k", "600k", "800k"))+
  ggtitle("Variable Months.since.last.delinquent posterior al filtro (valores < 85)")+
  theme_bw()

plot_2 = ggplot(Train_copy_continuas, aes(y=1, x=Months.since.last.delinquent))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  scale_y_continuous(NULL, labels = NULL)+
  # scale_x_continuous(breaks = c(0, 20000000, 40000000, 60000000, 80000000, 100000000),
  #                    labels = c("0", "20M", "40M", "60M", "80M", "100M"))+
  ggtitle("Variable Months.since.last.delinquent previo al filtro")+
  theme_bw()

grid.arrange(plot_2, plot_1, ncol = 1)
```

2do grupo:
`Maximum.Open.Credit` y
`Current.Loan.Amount`


```{r, echo=FALSE}
train_selected = select(Train_copy_continuas, Maximum.Open.Credit, Current.Loan.Amount)
train_long_form1 = gather(train_selected, key = "variable", value = "valor")

ggplot(train_long_form1, aes(y=variable, x=valor))+
  geom_boxplot()+
  # geom_jitter()+
  stat_summary(fun=mean, colour="darkred", geom="point",
               shape=18, size=3, show.legend=FALSE)+
  scale_x_continuous(NULL, breaks = c(0,  100000000, 200000000, 400000000, 600000000, 800000000),
                     labels = c("0", "100M", "200M", "400M","600M", "800M"))+
  scale_y_discrete(NULL)+
  theme_bw()
```
Se sospecha visualmente que en Current.Loan.Amount existe un dato outlier muy marcado cercano a los 100M, pero dicho outlier, no solo es un dato sino que pertenece a un subconjunto que representa el 4to cuartil con 4712 obs (13.77% del total).

Por lo tanto se vuelve caso de estudio análisar este subgrupo de outliers para evaluar la permanencia de estos registros en el ajuste del modelo.

- `Current.Loan.Amount`

Visualizamos los cuartiles de la variable `Current.Loan.Amount`

```{r, echo=FALSE}
quantiles_("Current.Loan.Amount")
```

En este punto cabe destacar que el 75% de los valores del vector Current.Loan.Amount son menores o iguales a 544,016.2. 

Dado que parece que los valores del cuarto cuartil son tan extremos para sugerirnos filtrarlos del estudio, pero en conteo representan 8,551 observaciones, se decide hacer un estudio de la distribución de los datos en el cuarto cuartil para evaluar si se pueden recuperar algunas observaciones. Es claro que un modelo no se va ajustar apropiadamente a registros con valores de cantidad de deuda actual de 500 mil y de 90 millones en una de las variables regresoras, y si se ajusta, no será el mejor modelo o la variable perdera significancia estadística en el modelo. 

En el análisis del cuarto cuartil, que son valores mayores a 544,016.2, se puede observar la siguiente distribución de los datos con ayuda de un gráfico de violin:


```{r, echo=FALSE}
df_temp = filter(Train_copy_continuas, Current.Loan.Amount > 544016.2 )
plot1 = ggplot(df_temp, aes(y=1,x=Current.Loan.Amount))+
  geom_violin()+
  scale_x_continuous(breaks = c(545000, 24500000, 76000000, 90000000),
                     labels = c("0.545M", "24.5M", "76M", "90M"))+
  scale_y_continuous(NULL, labels = NULL)+
  ggtitle("Análisis del 4to cuartil de la variable Current.Loan.Amount (valores > 544,016.2)")+
  theme_bw()


df_temp = filter(Train_copy_continuas, Current.Loan.Amount > 544016.2 )
plot2 = ggplot(df_temp, aes(Current.Loan.Amount))+
  geom_freqpoly()+
  scale_x_continuous(breaks = c(545000, 24500000, 76000000, 90000000),
                     labels = c("0.545M", "24.5M", "76M", "90M"))+
  ggtitle("Análisis del 4to cuartil de la variable Current.Loan.Amount (valores > 544,016.2)")+
  theme_bw()

grid.arrange(plot1, plot2, ncol = 1)
```
Y dado que los valores de la izquierda de la curva bimodal no son tan extremos como los de la parte derecha, nos planteamos la idea de tomar los de la izquierda y filtrar los valores extremos de la derecha. Dichos valores de la curva izquierda bimodal toman la siguiente distribución, con valor mínimo de 544,016.2 y máximo de 24.5 M, como no lo indica la anterior gráfia, que en conteo son 3,839 observaciones y tienen la siguiente distribución.

```{r, echo=FALSE}
df_temp = filter(Train_copy_continuas, Current.Loan.Amount > 544016.2 )
df_temp = filter(df_temp, Current.Loan.Amount < 24500000 )

ggplot(df_temp, aes(y=1, x=Current.Loan.Amount))+
  geom_violin()+
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  geom_jitter(alpha=0.2)+
  scale_y_continuous(NULL, labels = NULL)+
  scale_x_continuous(breaks = c(550000, 600000, 650000, 700000, 750000, 800000),
                     labels = c("550k", "600k", "650k", "700k", "750k", "800k"))+
  ggtitle("Análisis del 4to cuartil de la variable Current.Loan.Amount (valores > 544,016.2)")+
  theme_bw()
```

Por lo que en conclusión, en el análisis univariado de la variable `Current.Loan.Amount` y al poder observar la cota superior aproximada a la que debemos delimitarla para restringir valores extremos, el modelo se ajustara con todas aquellas obervaciones que sean menores o iguales a 800,000 (por redondeo), que representaran un 86.2% de las obervaciones de esta variable y que adoptan la siguiente distribución.

```{r, echo=FALSE}
plot_1 = ggplot(filter(Train_copy_continuas, Current.Loan.Amount <= 80000000), aes(y=1, x=Current.Loan.Amount))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  scale_y_continuous(NULL, labels = NULL)+
  scale_x_continuous(breaks = c(0, 200000, 400000, 600000, 800000),
                     labels = c("0", "200k", "400k", "600k", "800k"))+
  ggtitle("Variable Current.Loan.Amount posterior al filtro (valores < 80M)")+
  theme_bw()

plot_2 = ggplot(Train_copy_continuas, aes(y=1, x=Current.Loan.Amount))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  scale_y_continuous(NULL, labels = NULL)+
  scale_x_continuous(breaks = c(0, 20000000, 40000000, 60000000, 80000000, 100000000),
                     labels = c("0", "20M", "40M", "60M", "80M", "100M"))+
  ggtitle("Variable Current.Loan.Amount previo al filtro")+
  theme_bw()

grid.arrange(plot_2, plot_1, ncol = 1)
```

Finalmente, se procede a recopilar los IDs de dichos registros para al finalizar el análisis exploratorio, hacer el filtro del dataset inicial ya que podría darse el caso que en otras variables se ajuste otro filtro.


Por otro y sin ahondar mucho en la variable `Maximum.Open.Credit`, para eliminar valores extremos en el mismo sentido de la variable previa `Current.Loan.Amount` se establece la cota superior en `80M`. Esto bajo la lógica de que, si el tope máximo de la deuda actual es 80M es porque tienes un crédito otorgado máximo por esta cantidad. Este filtro posiblemente cambie en busqueda de un mejor ajuste del modelo, dado que el análisis de cuartiles nos indica que el 3er cuartil de `Maximum.Open.Credit` es un valor menor o igual a 697,531, sin embargo la lógica antes mencionada impera hasta este momento dle análisis.

```{r}
quantiles_("Maximum.Open.Credit")
```

3er grupo:
- `Annual.Income`

Visualizando los cuantiles de `Annual.Income`.

```{r}
quantiles_("Annual.Income")
```

Destacar que el 75% de los valores del vector Annual.Income son menores o iguales a 1,726,247 El número de observaciones en el 4to cuartil es de 8551 con un valor máximo de 30,838,994, por lo que un filtro eliminando toda esta submuestra nos haría perder una buena parte de la muestra. Por lo que decidimos hacer un análisis más a fondo para encontrar el punto en donde solo filtremos los valores que sean más extremos.


Con el apoyo visual de la anterior gráfica podemos decidir filtrar todas aquellas observaciones de la variable Annual.Income que sean mayores a 4000000 De esta forma, eliminariamos 572 variables de la muestra, representando un 1.67% de la misma.

```{r, echo=FALSE}
nrow(filter(Train_copy_continuas, Annual.Income > 4000000))
```
```{r, echo=FALSE}
plot_1 = ggplot(filter(Train_copy_continuas, Annual.Income <= 4000000), aes(y=1, x=Annual.Income))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  scale_y_continuous(NULL, labels = NULL)+
  scale_x_continuous(breaks = c(0, 1000000, 2000000, 3000000, 4000000, 5000000),
                     labels = c("0", "1M", "2M", "3M", "4M", "5M"))+
  ggtitle("Variable Annual.Income posterior al filtro (valores < 4000000)")+
  theme_bw()

plot_2 = ggplot(Train_copy_continuas, aes(y=1, x=Annual.Income))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  scale_y_continuous(NULL, labels = NULL)+
  scale_x_continuous(breaks = c(0, 5000000, 10000000, 20000000, 30000000),
                     labels = c("0", "5M", "10M", "20M", "30M"))+
  ggtitle("Variable Annual.Income previo al filtro")+
  theme_bw()

grid.arrange(plot_2, plot_1, ncol = 1)
```

4to grupo:
- `Current.Credit.Balance`

```{r}
quantiles_("Current.Credit.Balance")
```


En este punto cabe destacar que el 75% de los valores del vector Current.Credit.Balance son menores o iguales a 327,180.4 El número de observaciones en el 4to cuartil es de 8,552, por lo que un filtro eliminando toda esta submuestra nos haría perder una buena parte de la muestra. Por lo que decidimos hacer un análisis más a fondo para encontrar el punto en donde solo filtremos los valores que sean más extremos.

Con el apoyo visual de la siguiente gráfica pudimos decidir filtrar todas aquellas observaciones de la variable Current.Credit.Balance que sean mayores a 1000000 De esta forma, eliminariamos 588 variables de la muestra, representando un 1.71% de la misma.

```{r, echo=FALSE, include=FALSE}
nrow(filter(Train_copy_continuas, Current.Credit.Balance > 1000000))
```
```{r, echo=FALSE}
plot_1 = ggplot(filter(Train_copy_continuas, Current.Credit.Balance <= 1000000), aes(y=1, x=Current.Credit.Balance))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  scale_y_continuous(NULL, labels = NULL)+
  scale_x_continuous(breaks = c(0, 250000, 500000, 750000),
                     labels = c("0", "250K", "500K", "750K"))+
  ggtitle("Variable Current.Credit.Balance posterior al filtro (valores < 1000000)")+
  theme_bw()

plot_2 = ggplot(Train_copy_continuas, aes(y=1, x=Current.Credit.Balance))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  scale_y_continuous(NULL, labels = NULL)+
  scale_x_continuous(breaks = c(0, 2000000, 4000000, 6000000),
                     labels = c("0", "2M", "4M", "6M"))+
  ggtitle("Variable Annual.Income previo al filtro")+
  theme_bw()

grid.arrange(plot_2, plot_1, ncol = 1)
```

- `Monthly.Debt`

Visualizando los quantiles de `Monthly.Debt`

```{r}
quantiles_("Monthly.Debt")
```

En este punto cabe destacar que el 75% de los valores del vector Monthly.Debt son menores o iguales a 24,554.97 El número de observaciones en el 4to cuartil es de 8,553, por lo que un filtro eliminando toda esta submuestra nos haría perder una buena parte de la muestra. Por lo que decidimos hacer un análisis más a fondo para encontrar el punto en donde solo filtremos los valores que sean más extremos.

Con el apoyo visual de la siguiente gráfica pudimos decidir filtrar todas aquellas observaciones de la variable Current.Credit.Balance que sean mayores a 50,000 De esta forma, eliminariamos 781 variables de la muestra, representando un 2.28% de la misma.

```{r, echo=FALSE, include=FALSE}
nrow(filter(Train_copy_continuas, Monthly.Debt > 50000))
```
```{r}
plot_1 = ggplot(filter(Train_copy_continuas, Monthly.Debt <= 50000), aes(y=1, x=Monthly.Debt))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  scale_y_continuous(NULL, labels = NULL)+
  scale_x_continuous(breaks = c(0, 10000, 20000, 30000, 40000, 50000),
                     labels = c("0", "10K", "20K", "30K", "40K", "50K"))+
  ggtitle("Variable Monthly.Debt posterior al filtro (valores < 50000)")+
  theme_bw()

plot_2 = ggplot(Train_copy_continuas, aes(y=1, x=Monthly.Debt))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  scale_y_continuous(NULL, labels = NULL)+
  scale_x_continuous(breaks = c(0, 25000, 50000, 100000, 150000, 200000),
                     labels = c("0", "25K", "50K", "100K", "150K", "200K"))+
  ggtitle("Variable Annual.Income previo al filtro")+
  theme_bw()

grid.arrange(plot_2, plot_1, ncol = 1)
```

6to grupo:
- `Credit.Score`

La situación de la distribución de los valores parece ser un poco similar al caso de la variable `Current.Loan.Amount`, por lo que se buscara filtrar los registros de aquellas observaciones extremas para la variable Credit.Score en estudio. Cuando la media el score crediticio se encuentra en 1048.483, los valores de esta variable mayores a 5mil empiezan a perder mucho sentido, generandonos dudas sobre si estos datos se recopilaron apropiadamente o son inexactos como para ponerles un tope.

Por un lado se análizan los cuartiles del vector y se elaboran dos gráficas de cajas y bigotes más una capa de violin, además de un histógrama: una para valores que se encuentre en el primer, segundo y tercer cuartil y otra para el cuarto cuartil.

```{r}
quantiles_("Credit.Score")
```

```{r}
train_selected = Train_copy_continuas %>% filter(Credit.Score <= 738) %>% select(Credit.Score)
train_long_form8 = gather(train_selected, key = "variable", value = "valor")

plot_1 = ggplot(train_long_form8, aes(y=variable, x=valor))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  # geom_jitter()+ 
  stat_summary(fun=mean, colour="darkred", geom="point",
               shape=18, size=3, show.legend=FALSE)+
  scale_y_discrete(NULL)+
  ggtitle(glue("Credit.Score posterior al filtro (valores menores o iguales a 753)"))+
  theme_bw()

train_selected = Train_copy_continuas %>% select(Credit.Score)
train_long_form9 = gather(train_selected, key = "variable", value = "valor")

plot_2 = ggplot(train_long_form9, aes(y=variable, x=valor))+
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5)+
  # geom_jitter()+ 
  stat_summary(fun=mean, colour="darkred", geom="point",
               shape=18, size=3, show.legend=FALSE)+
  scale_y_discrete(NULL)+
  ggtitle(glue("Credit.Score previo al filtro"))+
  theme_bw()

grid.arrange(plot_2, plot_1, ncol = 1)
```

7mo grupo:
`Tax.Liens` y
`Bankruptcies`

Se decide no emplear filtros para este grupo.

A resaltar del análisis univariado y bivariado:

EN EL PERIODO DEL CREDITO
- Son más los clientes que solicitan un crédito a corto plazo (70%) que los que solicitan a LP (30%)
- Aquellos clientes con default son el grupo predominante en el gpo de clientes que solicitaron un  crédito a largo plazo
- La mediana del current loan es más alta para clientes con creditos CP que a LP, corroborar la media porque se notan outliers altos en CP.
- Son más los clientes con una hipóteca que piden crédito a LP que a CP, que los clientes que rentan su vivienda con créditos a CP que a LP. (variacion > 10%)
- La media de la deuda actual del cliente es mayor para los clientes que solicitan un crédito a LP, que uno a CP, lo cual tiene logica de negocio.
- La media del credit score es mayor para los clientes que solicitan crédito a CP que a LP.


EN LOS AÑOS DE TRABAJO DEL CLIENTE:
- El grupo que predomina es el de > 10 años de trabajo con un 35%, seguido de 2 años con un 8.5% 


EN EL DUEÑO DE LA VIVIENDA:
- Destaca el grupo de clientes con hipóteca con un 51%, seguido de Renta con un 39% y solo casa propia con un 9%
- Hay dos factores que se podrían unificar: have mortgage & home mortgage, aunque la prop del primeor es de 0.2% por lo que no vale la pena

EN EL PROPOSITO DEL CRÉDITO:
- Destaca con un 73% el proposito de consolidación de la deuda como proposito declarado. 

EN LAS BANCARROTAS:
- El 65% de los clientes declara nunca haber caído en bancarrota, le sigue el 25% con al menos una bancarrota.
- El grupo, por proposito del crédito, que predomina en una bancarrota declarada es aquel grupo que declara solicitar el crédito para "moving"

EN LA DEUDA ACTUAL DEL CLIENTE:
- La mediana de la deuda tiene una relación negativa con el aumento en el credit scoring, a mayor credit score menor es la mediana (falta corroborar la media dado outliers en altos credit scores)
- la mediana sigue una relacion positiva con el ingreso anual (corroborar media)
- la mediana sigue una relacion positiva con la deuda mensual (corroborar media)
- la mediana sigue una relacion positiva con el balance crediticio (corroborar media)

EN LOS AÑOS DE HISTORIAL CREDITICIO:

- La mediana de los años mantiene una relacion positiva con el balance crediticio

EN EL BALANCE ACTUAL DEL CREDITO:
- La mediana del balance mantiene una relacion positiva con el maximo de creditos abiertos.

EN EL NUMERO DE BANCARROTAS:

- La mediana del numero máximo de creditos abiertos se relaciona negativamente con el número de bancarrotas. Entre más bancarrotas es menor el numero de créditos abiertos.


#### Análisis multivariado

```{r}

```


En este apartado generamos los nuevos datasets a estudiar en siguientes scripts


- Modelado con toda la muestra.
- Modelado con una muestra reducida (filtro en dos variables continuas)
- Modelado con una muestra aun más reducida (filtro en x variables continuas)


Muestra completa
```{r}
write_csv(Train_copy, "../datasets/muestra_completa.csv")
```

```{r}
Train_copy_1_continues = Train_copy %>% select(Current.Loan.Amount, Credit.Score, Annual.Income, Monthly.Debt, Years.of.Credit.History, Months.since.last.delinquent, Number.of.Open.Accounts, Current.Credit.Balance, Maximum.Open.Credit, Bankruptcies, Tax.Liens)

chart.Correlation(select(Train_copy_1_continues, -c(Tax.Liens)), histogram=TRUE, pch=19, main = "Matriz de correlaciones con histógrama muestra completa")
```


Muestra reducida no. 1, en donde se propondrá un modelo con un 81% de la muestra original
```{r}
Train_copy_1 = Train_copy %>% filter(Current.Loan.Amount <= 80000000, Credit.Score <= 753, Maximum.Open.Credit <= 80000000)

print(glue("Train_copy en un inicio tuvó {format(nrow(Train_copy), big.mark = ',')} obs"))
print(glue("Train_copy_1 ahora tiene {format(nrow(Train_copy_1), big.mark = ',')} obs"))
print(glue("Se redujo el dataset en un {(1 - (nrow(Train_copy_1)/nrow(Train_copy)))*100 }%"))

write_csv(Train_copy_1, "../datasets/muestra_reducida_1.csv")
```
```{r}
Train_copy_1_continues = Train_copy_1 %>% select(Current.Loan.Amount, Credit.Score, Annual.Income, Monthly.Debt, Years.of.Credit.History, Months.since.last.delinquent, Number.of.Open.Accounts, Current.Credit.Balance, Maximum.Open.Credit, Bankruptcies, Tax.Liens)

chart.Correlation(select(Train_copy_1_continues, -c(Tax.Liens)), histogram=TRUE, pch=19, main = "Matriz de correlaciones con histógrama muestra reducida no. 1")
```


Muestra reducida no. 2, en donde se propondrá un modelo con el 32% de la muestra original.
```{r}
Train_copy_2 = Train_copy %>% filter(Current.Loan.Amount <= 80000000, Credit.Score <= 753, Maximum.Open.Credit <= 80000000, Years.of.Credit.History <= 40, Number.of.Open.Accounts <= 30, Months.since.last.delinquent <= 30, Current.Loan.Amount <= 80000000, Annual.Income <= 4000000, Current.Credit.Balance <= 1000000, Monthly.Debt <= 50000, Credit.Score <= 753)

print(glue("Train_copy en un inicio tuvó {format(nrow(Train_copy), big.mark = ',')} obs"))
print(glue("Train_copy_2 ahora tiene {format(nrow(Train_copy_2), big.mark = ',')} obs"))
print(glue("Se redujo el dataset en un {(1 - (nrow(Train_copy_2)/nrow(Train_copy)))*100 }%"))

write_csv(Train_copy_2, "../datasets/muestra_reducida_2.csv")
```

```{r}
Train_copy_1_continues = Train_copy_2 %>% select(Current.Loan.Amount, Credit.Score, Annual.Income, Monthly.Debt, Years.of.Credit.History, Months.since.last.delinquent, Number.of.Open.Accounts, Current.Credit.Balance, Maximum.Open.Credit, Bankruptcies, Tax.Liens)

chart.Correlation(select(Train_copy_1_continues, -c(Tax.Liens)), histogram=TRUE, pch=19, main = "Matriz de correlaciones con histógrama muestra reducida no. 2")
```


La continuación de este proyecto, que es la parte del modelado, continuará en los rmarkdown modelo 1, modelo 2 y modelo 3 respectivamente. En cada uno de estos rmarkdowns donde se plantea ajustar el mejor modelo dada la submuestra analizada, separando Train y Test, empleando selección de variables, validación cruzada y bootstrap, finalizando con generar un archivo RData de cada modelo así como un diagnostico con el estadístico PRESS.

La idea principal de esto es llevar cada modelo a una validación cruzada simulada con la muestra Test que será evaluada en el proyecto final de clase.



















